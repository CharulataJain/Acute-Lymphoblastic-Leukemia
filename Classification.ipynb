{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis of Acute Lymphomatic Leukemia with Various Classification Algorithms \n",
    "\n",
    "This notebook covers the code and accuracy reports for the following algorithms. For further information about the data and the problem statement please view the [README.md](https://github.com/GV-9wj/Acute-Lymphomatic-Leukemia-ALL-IDB-prediction/blob/main/README.md) file.\n",
    "\n",
    "##### Classification Algorithms used in this notebook are:\n",
    "\n",
    "1. K Nearest Neighbours Classifier [KNN sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "2. Support Vector Machine Classifier [SVC sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "3. Naive Bayes Classifier [GNB sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "\n",
    "##### Process being followed in this notebook\n",
    "The process we will follow to use the images for training and testing are:\n",
    "\n",
    "1. Importing the necessary libraries\n",
    "\n",
    "\n",
    "2. Defining functions that will:\n",
    "    1. Convert the image into an array of pixel intensities\n",
    "    2. Convert the image into a normalized form of the colour histogram of the image\n",
    "\n",
    "\n",
    "3. Deifne the variables we will be using for train and test\n",
    "    1. The variable **rawImages** will act as the data we use for predicting the labels using raw image data (pixel data)\n",
    "    2. The **features** list will act as the data we use for predicting the labels using the histogram data.\n",
    "    3. The **labels** list will act as the predictor variable for our problem statement\n",
    "\n",
    "\n",
    "4. Read the files one by one and for each file and Compute the aformentioned variables and append it to the list one by one.\n",
    "\n",
    "\n",
    "5. Split the data into train and test data.\n",
    "    1. One split for the raw pixel data\n",
    "    2. Another split for the histogram data.\n",
    "\n",
    "\n",
    "6. Train and Test the models, one for each type of variable\n",
    "    1. For Training we will use the following classifiers\n",
    "        1. K-Nearest Neigbours \n",
    "        2. Support Vector Classifier\n",
    "        3. Naive Bayes Classifier\n",
    "    2. For Testing the data and then evaluvating it we will use the following metrics\n",
    "        1. *model.score()*\n",
    "        2. A classification report that gives us the precision and the recall and also the F1 score, where:\n",
    "            1. *Precision* is the fraction of how many of the items that are selected are relevant.\n",
    "            2. *Recall* is the fraction of how many relevant items are selected.\n",
    "            3. *F1 score* which is the harmonic mean between the Precision and the Recall\n",
    "        3. A *confusion matrix* which gives us a matrix of the true negatives($C_{0, 0}$), false negatives($C_{1, 0}$), true positives($C_{0, 1}$) false positives($C_{1, 1}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Importing the necessary libraries\n",
    "\n",
    "We will first import the basic functions required for image processing. These funtions can be found in the modules [`cv2`](https://github.com/skvark/opencv-python) and [`imutils`](https://github.com/jrosebr1/imutils) both of which are used for basic image processing. <br /> \n",
    "Then we shall import the functions required to read through files from the `os` module. The function used in this notebook is [`listdir` ](https://docs.python.org/3/library/os.html#os.listdir). <br />\n",
    "Finally we will import the classifiers required for the project, namely KNN, SVM and Naive Bayes, and also import the modules required for training and testing the data from [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import os \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB as gnb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Defining the functions required to change the images into machine readable format.\n",
    "There are two functions that are being used in this kernel, `image_to_feature_vector` and also `extract_color_histogram`. Let us look at them in detail\n",
    "\n",
    "#### 1. `image_to_feature_vector`\n",
    "This function takes into arguments the image file and the size (resolution) of the image. We will first resize the image into a 32X32 grid and hence change the dimensions of the image using the [`cv2.resize`](https://www.tutorialkart.com/opencv/python/opencv-python-resize-image/) function. This function takes the image, resizes it into the desired size, in our case 32X32, and then for each grid in the 32X32 image (total of 1024) it extracts the RGB value ([What is RGB](https://www.scantips.com/basics1b.html#:~:text=A%20digital%20color%20image%20pixel,Red%2C%20Green%2C%20Blue)). This means it returns 3 values for each and every grid. Finally this 32X32X3 array is flattened using the `.flatten()` function, part of the numpy module, to flatten out the array into a numpy array and this numpy array holds a total of 3072 numbers. \n",
    "\n",
    "![Fig. 1 Flow Chart for the `image_to_feature_vector` function](data/FlowFunc1.jpg)\n",
    "\n",
    "#### 2. `extract_color_histogram`\n",
    "This function takes into arguments the  image and the bins into which we want to split the colour histogram. The first part of the function `cv2.cvtColor()` takes the IMAGE converts it into HSV workspace.<br />\n",
    "The second function we use `cv2.calchist()`converts the workspace into a 3-DHistogram [(What is a 3-D Histogram)](http://dofideas.com/h3stogram-interactive-3d-color-histogram-en/#:~:text=In%20image%20processing%20and%20photography,set%20of%20all%20possible%20colors.).<br />\n",
    "In this function we use the HSV image and the 0, 1, 2, in the second argument refers to the channels. Since we want to work with RGB we are using 3 channels and naming them 0,1, 2 respectively. The bins we take are 8 each, this means we want the values of intensity histogram for every 8 pixels of the image. The next argument is the ranges for the workspace and since we are on the HSV workspace we use the ranges of [0, 256] for each channel.<br />\n",
    "The `is_cv2()` function is used to check the version of OpenCv because OpenCv handles normalization in one way for OpenCv 2.4.x and another way for OpenCv 3. [(What is Normalization?)](https://en.wikipedia.org/wiki/Normalization_(image_processing)#:~:text=In%20image%20processing%2C%20normalization%20is,range%20of%20pixel%20intensity%20values.&text=The%20purpose%20of%20dynamic%20range,senses%2C%20hence%20the%20term%20normalization.)<br />\n",
    "Finally after normalization we flatten it to a numpy array for machine readablity.\n",
    "\n",
    "![Fig. 1 Flow Chart for the `extract_color_histogram` function](data/FlowFunc2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_feature_vector(image, size=(32, 32)):\n",
    "    # resize the image to a fixed size, then flatten the image into\n",
    "    # a list of raw pixel intensities\n",
    "    return cv2.resize(image, size).flatten()\n",
    "\n",
    "\n",
    "\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    # extract a 3D color histogram from the HSV color space using\n",
    "    # the supplied number of `bins` per channel\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    # handle normalizing the histogram if we are using OpenCV 2.4.X\n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist)\n",
    "    # otherwise, perform \"in place\" normalization in OpenCV 3\n",
    "    else:\n",
    "        cv2.normalize(hist, hist)\n",
    "    # return the flattened histogram as\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Deifning the variables to be used in model training and testing\n",
    "Within this step we will define 3 variables, **rawImages, features,** and **labels**. The rawImage vector will take the output from the `image_to_feature_vector` function. The features variable will take the output from the `extract_color_histogram` function. Finally the labels variable will carry the labels of the person, 0 if they are healthy and 1 if they are suffering from leukemia. This can be extracted from the file name. The reason we are looking at two independent variables is so that we compare the results of using pixel data alone or colour histogram data which holds color intensity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawImages = []\n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Assigning values to the predefinied variables\n",
    "In this step we will be using the pre-defined functions and loop through the files to assign values to them. The function `os.listdir` is used to get a list of all the files in the folder. We will then loop through the list and read each and every file into an image. Finally we will use the functions to generate the two major variables, **rawImages** and **features**. The **labels** variable, which is our predictor variable can be derived using image-file name's last character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datapath = 'Data/ALL_IDB2/img/'\n",
    "imagePaths = os.listdir(Datapath)\n",
    "\n",
    "# loop over the input images\n",
    "for file in imagePaths:\n",
    "    # load the image and extract the class label (assuming that our\n",
    "    # path as the format: /path/to/dataset/{class}.{image_num}.jpg\n",
    "    image = cv2.imread(Datapath + file)\n",
    "    label = int(file[6])\n",
    "    # extract raw pixel intensity \"features\", followed by a color\n",
    "    # histogram to characterize the color distribution of the pixels\n",
    "    # in the image\n",
    "    pixels = image_to_feature_vector(image)\n",
    "    hist = extract_color_histogram(image)\n",
    "    # update the raw images, features, and labels matricies,\n",
    "    # respectively\n",
    "    rawImages.append(pixels)\n",
    "    features.append(hist)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just want to convert it into a numpy array for better machinability. We are also checking the size of our variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pixels matrix: 0.78MB\n",
      "[INFO] features matrix: 0.52MB\n"
     ]
    }
   ],
   "source": [
    "# show some information on the memory consumed by the raw images\n",
    "# matrix and features matrix\n",
    "rawImages = np.array(rawImages)\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "print(\"[INFO] pixels matrix: {:.2f}MB\".format(\n",
    "    rawImages.nbytes / (1024 * 1000.0)))\n",
    "print(\"[INFO] features matrix: {:.2f}MB\".format(\n",
    "    features.nbytes / (1024 * 1000.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 260, 260)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawImages), len(features), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Splitting the data into train and test\n",
    "Using the function `train_etst_split()` we will split the dataset into train data and test data.<br />\n",
    "Here we are conducting two splits, one for the Image Pixed data along with its labels and another with the Image's color histogram data along with the labels of the images. This way we will need to train and evaluvate our model for the raw pixel intensities and also for the color histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits, using 75%\n",
    "# of the data for training and the remaining 25% for testing\n",
    "(trainImage, testImage, trainImageLabel, testImageLabel) = train_test_split(rawImages, \n",
    "                                                                            labels, \n",
    "                                                                            test_size=0.25,\n",
    "                                                                            random_state = 123)\n",
    "(trainFeat, testFeat, trainFeatLabel, testFeatLabel) = train_test_split(features, \n",
    "                                                                        labels, \n",
    "                                                                        test_size=0.25, \n",
    "                                                                        random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 Training, Testing and evaluvating the classifiers\n",
    "Within this step we will be calling the classifiers and then training them using the data defined above and finally testing them. This process will be repetitive for each and every classifier, and for each classifier we will be doing it twice one for the raw pixel data and another for the color histograms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K - Nearest Neighbours classifier\n",
    "In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the Raw-Pixel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating raw pixel accuracy for KNN...\n",
      "\n",
      "K-Nearest Neighbours Test Accuracy for image data: 73.84615384615385 %\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71        27\n",
      "           1       0.82      0.71      0.76        38\n",
      "\n",
      "    accuracy                           0.74        65\n",
      "   macro avg       0.74      0.74      0.74        65\n",
      "weighted avg       0.75      0.74      0.74        65\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      " Confusion Matrix :- \n",
      "[[21  6]\n",
      " [11 27]]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate a k-NN classifer on the raw pixel intensities\n",
    "print(\"[INFO] evaluating raw pixel accuracy for KNN...\")\n",
    "knn_model_image = KNN()\n",
    "knn_model_image.fit(trainImage, trainImageLabel)\n",
    "\n",
    "y_pred_knn_image = knn_model_image.predict(testImage)\n",
    "\n",
    "print(\"\\nK-Nearest Neighbours Test Accuracy for image data: {} %\".\n",
    "      format(metrics.accuracy_score(testImageLabel,\n",
    "                                            y_pred_knn_image)*100))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nClassification Report : \\n {}\".format(metrics.classification_report(testImageLabel, \n",
    "                                                                                     y_pred_knn_image)))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n Confusion Matrix :- \\n{}\".format(metrics.confusion_matrix(testImageLabel, \n",
    "                                                                            y_pred_knn_image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the Colour histogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating histogram accuracy for KNN...\n",
      "\n",
      "K-Nearest Neighbours Test Accuracy for image data: 92.3076923076923 %\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92        30\n",
      "           1       0.97      0.89      0.93        35\n",
      "\n",
      "    accuracy                           0.92        65\n",
      "   macro avg       0.92      0.93      0.92        65\n",
      "weighted avg       0.93      0.92      0.92        65\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      " Confusion Matrix :- \n",
      "[[29  1]\n",
      " [ 4 31]]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate a k-NN classifer on the histogram\n",
    "# representations\n",
    "print(\"[INFO] evaluating histogram accuracy for KNN...\")\n",
    "knn_model_hist = KNN()\n",
    "knn_model_hist.fit(trainFeat, trainFeatLabel)\n",
    "\n",
    "y_pred_knn_hist = knn_model_hist.predict(testFeat)\n",
    "\n",
    "print(\"\\nK-Nearest Neighbours Test Accuracy for Colour histogram data: {} %\".\n",
    "      format(metrics.accuracy_score(testFeatLabel,\n",
    "                                            y_pred_knn_hist)*100))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nClassification Report : \\n {}\".format(metrics.classification_report(testFeatLabel, \n",
    "                                                                                     y_pred_knn_hist)))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n Confusion Matrix :- \\n{}\".format(metrics.confusion_matrix(testFeatLabel, \n",
    "                                                                            y_pred_knn_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Classifier\n",
    "A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text. So you're working on a text classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the Raw-Pixel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating raw pixel accuracy for SVC...\n",
      "\n",
      "Support Vector Machine Test Accuracy for image data: 76.92307692307693 %\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.72        27\n",
      "           1       0.79      0.82      0.81        38\n",
      "\n",
      "    accuracy                           0.77        65\n",
      "   macro avg       0.76      0.76      0.76        65\n",
      "weighted avg       0.77      0.77      0.77        65\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      " Confusion Matrix :- \n",
      "[[19  8]\n",
      " [ 7 31]]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate a k-NN classifer on the raw pixel intensities\n",
    "print(\"[INFO] evaluating raw pixel accuracy for SVC...\")\n",
    "svc_model_image = SVC()\n",
    "svc_model_image.fit(trainImage, trainImageLabel)\n",
    "\n",
    "y_pred_svc_image = svc_model_image.predict(testImage)\n",
    "\n",
    "print(\"\\nSupport Vector Machine Test Accuracy for image data: {} %\".\n",
    "      format(metrics.accuracy_score(testImageLabel,\n",
    "                                            y_pred_svc_image)*100))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nClassification Report : \\n {}\".format(metrics.classification_report(testImageLabel, \n",
    "                                                                                     y_pred_svc_image)))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n Confusion Matrix :- \\n{}\".format(metrics.confusion_matrix(testImageLabel, \n",
    "                                                                            y_pred_svc_image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the Colour histogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating histogram accuracy for SVC...\n",
      "\n",
      "Support Vector Machine Test Accuracy for image data: 87.6923076923077 %\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88        30\n",
      "           1       1.00      0.77      0.87        35\n",
      "\n",
      "    accuracy                           0.88        65\n",
      "   macro avg       0.89      0.89      0.88        65\n",
      "weighted avg       0.90      0.88      0.88        65\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      " Confusion Matrix :- \n",
      "[[30  0]\n",
      " [ 8 27]]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate a k-NN classifer on the histogram\n",
    "# representations\n",
    "print(\"[INFO] evaluating histogram accuracy for SVC...\")\n",
    "svc_model_hist = SVC()\n",
    "svc_model_hist.fit(trainFeat, trainFeatLabel)\n",
    "\n",
    "y_pred_svc_hist = svc_model_hist.predict(testFeat)\n",
    "\n",
    "print(\"\\nSupport Vector Machine Test Accuracy for Colour histogram data: {} %\".\n",
    "      format(metrics.accuracy_score(testFeatLabel,\n",
    "                                            y_pred_svc_hist)*100))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nClassification Report : \\n {}\".format(metrics.classification_report(testFeatLabel, \n",
    "                                                                                     y_pred_svc_hist)))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n Confusion Matrix :- \\n{}\".format(metrics.confusion_matrix(testFeatLabel, \n",
    "                                                                            y_pred_svc_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier\n",
    "In statistics, Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong independence assumptions between the features. They are among the simplest Bayesian network models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the Raw-Pixel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating raw pixel accuracy for Naive Bayes...\n",
      "\n",
      "Naive Bayes Classifier Test Accuracy for image data: 72.3076923076923 %\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.48      0.59        27\n",
      "           1       0.71      0.89      0.79        38\n",
      "\n",
      "    accuracy                           0.72        65\n",
      "   macro avg       0.74      0.69      0.69        65\n",
      "weighted avg       0.73      0.72      0.71        65\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      " Confusion Matrix :- \n",
      "[[13 14]\n",
      " [ 4 34]]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate a k-NN classifer on the raw pixel intensities\n",
    "print(\"[INFO] evaluating raw pixel accuracy for Naive Bayes...\")\n",
    "gnb_model_image = gnb()\n",
    "gnb_model_image.fit(trainImage, trainImageLabel)\n",
    "\n",
    "y_pred_gnb_image = gnb_model_image.predict(testImage)\n",
    "\n",
    "print(\"\\nNaive Bayes Classifier Test Accuracy for image data: {} %\".\n",
    "      format(metrics.accuracy_score(testImageLabel,\n",
    "                                            y_pred_gnb_image)*100))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nClassification Report : \\n {}\".format(metrics.classification_report(testImageLabel, \n",
    "                                                                                     y_pred_gnb_image)))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n Confusion Matrix :- \\n{}\".format(metrics.confusion_matrix(testImageLabel, \n",
    "                                                                            y_pred_gnb_image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the Colour histogram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating histogram accuracy for Naive Bayes Classifer...\n",
      "\n",
      "Naive Bayes Classifier Test Accuracy for image data: 72.3076923076923 %\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.43      0.59        30\n",
      "           1       0.67      0.97      0.79        35\n",
      "\n",
      "    accuracy                           0.72        65\n",
      "   macro avg       0.80      0.70      0.69        65\n",
      "weighted avg       0.79      0.72      0.70        65\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      " Confusion Matrix :- \n",
      "[[13 17]\n",
      " [ 1 34]]\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate a k-NN classifer on the histogram\n",
    "# representations\n",
    "print(\"[INFO] evaluating histogram accuracy for Naive Bayes Classifer...\")\n",
    "gnb_model_hist = gnb()\n",
    "gnb_model_hist.fit(trainFeat, trainFeatLabel)\n",
    "\n",
    "y_pred_gnb_hist = gnb_model_hist.predict(testFeat)\n",
    "\n",
    "print(\"\\nNaive Bayes Classifier Test Accuracy for Colour histogram data: {} %\".\n",
    "      format(metrics.accuracy_score(testFeatLabel,\n",
    "                                            y_pred_gnb_hist)*100))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\nClassification Report : \\n {}\".format(metrics.classification_report(testFeatLabel, \n",
    "                                                                                     y_pred_gnb_hist)))\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\n Confusion Matrix :- \\n{}\".format(metrics.confusion_matrix(testFeatLabel, \n",
    "                                                                            y_pred_gnb_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Class 0 (Patient without Leukeamia) Image data\n",
    "\n",
    "|MODEL                      |Precision | Recall | f1 Score| Test accuracy|\n",
    "|---------------------------|----------|--------|---------|--------------|\n",
    "| ``K-Nearest Neigbours``   | 0.66     | 0.78   |0.71     | 73.84 %      |\n",
    "| ``Support Vector Machine``| 0.73     | 0.70   |0.72     | 76.92 %      |\n",
    "| `` Naive Bayes``          | 0.76     | 0.48   |0.59     | 72.30 %      |\n",
    "\n",
    "\n",
    "## Results for Class 1 (Patient with Leukeamia) Image Data\n",
    "\n",
    "|MODEL                      |Precision | Recall | f1 Score| Test accuracy|\n",
    "|---------------------------|----------|--------|---------|--------------|\n",
    "| ``K-Nearest Neigbours``   | 0.82     | 0.71   |0.76     | 73.84 %      |\n",
    "| ``Support Vector Machine``| 0.79     | 0.82   |0.81     | 76.92 %      |\n",
    "| `` Naive Bayes``          | 0.71     | 0.89   |0.79     | 72.30 %      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Class 0 (Patient without Leukeamia) Color Histogram data\n",
    "\n",
    "|MODEL                      |Precision | Recall | f1 Score| Test accuracy|\n",
    "|---------------------------|----------|--------|---------|--------------|\n",
    "| ``K-Nearest Neigbours``   | 0.88     | 0.97   |0.92     | 92.30 %      |\n",
    "| ``Support Vector Machine``| 0.79     | 1.00   |0.88     | 87.69 %      |\n",
    "| `` Naive Bayes``          | 0.93     | 0.43   |0.59     | 72.30 %      |\n",
    "\n",
    "72.3076923076923 %\n",
    "-------------------------------------------------------------------------------------------------\n",
    "\n",
    "Classification Report : \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0                   0.59        30\n",
    "           1       0.67      0.97      0.79        35\n",
    "\n",
    "## Results for Class 1 (Patient with Leukeamia) Color Histogram Data\n",
    "\n",
    "|MODEL                      |Precision | Recall | f1 Score| Test accuracy|\n",
    "|---------------------------|----------|--------|---------|--------------|\n",
    "| ``K-Nearest Neigbours``   | 0.97     | 0.89   |0.93     | 92.30 %      |\n",
    "| ``Support Vector Machine``| 0.79     | 0.82   |0.81     | 87.69 %      |\n",
    "| `` Naive Bayes``          | 1.00     | 0.77   |0.87     | 72.30 %      |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
